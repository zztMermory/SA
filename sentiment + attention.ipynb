{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结果\n",
    "model | acc（%） | total params |\n",
    "---|---|---|\n",
    "实验2：baseline(new method - attention)|85.47|6,000,301\n",
    "实验3：baseline(LSTM)|86.09|6,219,777\n",
    "实验4：baseline(LSTM+Attention)|89.08|6,232,777\n",
    "实验1：__new method__(embeddings+Attention)|__89.12__|6,020,251\n",
    "1. 实验1是提出的新模型，结果最优，参数少；<br/>实验2是去掉新模型中的注意力机制后的模型，baseline；<br/>实验3是LSTM模型,实验4是LSTM+attention模型，准确率和新模型不相上下，但是比新模型参数多200,000个左右。<br/>ps:实验3结果不稳定：出现过84%，87.22%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验\n",
    "#### 0 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Lambda,RepeatVector,Lambda,Reshape\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Bidirectional,GlobalAveragePooling1D,Input,GlobalMaxPooling1D,Multiply\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "from numpy.random import seed\n",
    "seed(20190419)    \n",
    "from tensorflow import set_random_seed \n",
    "set_random_seed(20190419)\n",
    "\n",
    "max_features= 20000\n",
    "maxlen=250\n",
    "batch_size=32\n",
    "embedding_dims=300\n",
    "epochs=20\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(\"F:/Anaconda3/word vector and sentiment analysis/data/imdb.npz\",num_words=max_features)\n",
    "\n",
    "x_train=sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(x_test,maxlen=maxlen)\n",
    "\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 新方法 embeddings+attention\n",
    "模型结构：&nbsp;input&nbsp; ->&nbsp; embeddings &emsp;&emsp;&emsp;&emsp;   -> &nbsp; embeddings&nbsp; * &nbsp; attention emb &nbsp;->&nbsp; sum &nbsp;->&nbsp; predict  &nbsp;  <br/>\n",
    "&emsp; &emsp;  &emsp;&emsp;&emsp;&emsp;&emsp; -> attention emb -> 归一化                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atten_L1 (Embedding)            (None, 250, 1)       20000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 250, 1)       0           atten_L1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     6000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 250, 300)     0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 250, 300)     0           embedding_1[0][0]                \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 250)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            251         lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,020,251\n",
      "Trainable params: 6,020,251\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.6789 - acc: 0.6371 - val_loss: 0.6178 - val_acc: 0.7930\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 269s 11ms/step - loss: 0.4874 - acc: 0.8305 - val_loss: 0.3959 - val_acc: 0.8428\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 248s 10ms/step - loss: 0.3535 - acc: 0.8612 - val_loss: 0.3347 - val_acc: 0.8622\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 234s 9ms/step - loss: 0.3081 - acc: 0.8784 - val_loss: 0.3090 - val_acc: 0.8712\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 247s 10ms/step - loss: 0.2807 - acc: 0.8898 - val_loss: 0.2951 - val_acc: 0.8783\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 257s 10ms/step - loss: 0.2617 - acc: 0.8976 - val_loss: 0.2853 - val_acc: 0.8823\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 253s 10ms/step - loss: 0.2454 - acc: 0.9059 - val_loss: 0.2785 - val_acc: 0.8853\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 235s 9ms/step - loss: 0.2311 - acc: 0.9119 - val_loss: 0.2735 - val_acc: 0.8870\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 239s 10ms/step - loss: 0.2181 - acc: 0.9180 - val_loss: 0.2703 - val_acc: 0.8888\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 235s 9ms/step - loss: 0.2057 - acc: 0.9238 - val_loss: 0.2678 - val_acc: 0.8896\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.1938 - acc: 0.9291 - val_loss: 0.2667 - val_acc: 0.8909\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 247s 10ms/step - loss: 0.1828 - acc: 0.9341 - val_loss: 0.2665 - val_acc: 0.8912\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.1729 - acc: 0.9393 - val_loss: 0.2674 - val_acc: 0.8898\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 250s 10ms/step - loss: 0.1627 - acc: 0.9434 - val_loss: 0.2690 - val_acc: 0.8898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16365bd25f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = Input(shape=(maxlen,), dtype='int32')\n",
    "input_vecs = Embedding(max_features, embedding_dims)(input_words)\n",
    "input_atten = Embedding(max_features,1,name='atten_L1')(input_words)\n",
    "input_atten_norm=Lambda(lambda x: K.exp(input_atten)/K.sum(K.exp(input_atten)))(input_atten)  # attention 归一化\n",
    "input_atten_1=Lambda(lambda x: K.repeat_elements(input_atten_norm, embedding_dims, 2))(input_atten_norm)\n",
    "temp_layers= Multiply()([input_vecs,input_atten_1])   # 单词 对应的 word vectors 和 attention 相乘\n",
    "sumLayer=Lambda(lambda x: K.sum(x, axis=2))(temp_layers)  # axis=1时，是单词词向量求和；此模型选择axis=2,是对每个单词词向量各项的相加，每个单词对应一个值，\n",
    "predict=Dense(1, activation='sigmoid')(sumLayer)\n",
    "model = Model(inputs=input_words, outputs=predict)\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "callbacks = EarlyStopping(monitor='val_loss', patience=2, verbose=0)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "         callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 baseline: embeddings （ \"no\" attention）\n",
    "模型结构：input -> embeddings -> sum -> predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 250, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 6,000,301\n",
      "Trainable params: 6,000,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.3898 - acc: 0.8473 - val_loss: 0.3688 - val_acc: 0.8492\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 189s 8ms/step - loss: 0.1726 - acc: 0.9337 - val_loss: 0.4122 - val_acc: 0.8547\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.1191 - acc: 0.9552 - val_loss: 0.4731 - val_acc: 0.8513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163479428d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = Input(shape=(maxlen,), dtype='int32')\n",
    "input_vecs = Embedding(max_features, embedding_dims)(input_words)\n",
    "sumLayer=Lambda(lambda x: K.sum(x, axis=1))(input_vecs)\n",
    "predict=Dense(1, activation='sigmoid')(sumLayer)\n",
    "model = Model(inputs=input_words, outputs=predict)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "callbacks = EarlyStopping(monitor='val_loss', patience=2, verbose=0)\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "         callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 baseline: embeddings + LSTM\n",
    "模型结构：input -> embeddings -> LSTM -> predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 250, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,219,777\n",
      "Trainable params: 6,219,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 855s 34ms/step - loss: 0.5465 - acc: 0.7217 - val_loss: 0.4330 - val_acc: 0.8010\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 858s 34ms/step - loss: 0.3390 - acc: 0.8588 - val_loss: 0.3789 - val_acc: 0.8362\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 901s 36ms/step - loss: 0.2446 - acc: 0.9034 - val_loss: 0.3759 - val_acc: 0.8593\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 872s 35ms/step - loss: 0.1977 - acc: 0.9244 - val_loss: 0.3957 - val_acc: 0.8344\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 894s 36ms/step - loss: 0.1482 - acc: 0.9464 - val_loss: 0.4306 - val_acc: 0.8609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163002f4630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = Input(shape=(maxlen,), dtype='int32')\n",
    "input_vecs = Embedding(max_features, embedding_dims)(input_words)\n",
    "lstm=LSTM(128, dropout=0.2, recurrent_dropout=0.2)(input_vecs)\n",
    "predict=Dense(1, activation='sigmoid')(lstm)\n",
    "model = Model(inputs=input_words, outputs=predict)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "callbacks = EarlyStopping(monitor='val_loss', patience=2, verbose=0)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "         callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 baseline: embeddings + LSTM\n",
    "模型结构：input -> embeddings -> LSTM -> attention -> predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 250, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 250, 128)          219648    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 128)               13000     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,232,777\n",
      "Trainable params: 6,232,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 886s 35ms/step - loss: 0.3536 - acc: 0.8371 - val_loss: 0.2641 - val_acc: 0.8908\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 934s 37ms/step - loss: 0.1694 - acc: 0.9371 - val_loss: 0.2971 - val_acc: 0.8786\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 954s 38ms/step - loss: 0.0923 - acc: 0.9684 - val_loss: 0.3475 - val_acc: 0.8664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16306d5d9e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    \n",
    "# maxlen=250\n",
    "input_words = Input(shape=(maxlen,), dtype='int32')\n",
    "input_vecs = Embedding(max_features, embedding_dims)(input_words)\n",
    "lstm=LSTM(128, dropout=0.2, recurrent_dropout=0.2,return_sequences = True)(input_vecs)\n",
    "attention = AttLayer(100)(lstm)\n",
    "predict=Dense(1, activation='sigmoid')(attention)\n",
    "model = Model(inputs=input_words, outputs=predict)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "callbacks = EarlyStopping(monitor='val_loss', patience=2, verbose=0)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "         callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
